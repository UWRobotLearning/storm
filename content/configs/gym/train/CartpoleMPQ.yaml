
expert_data_path: './tmp_results/${task_name}/MPC/models/agent_buffer_0.pt'

policy:
  mlp_params:
    hidden_layers: [256, 256] 
    activation: torch.nn.ReLU 
    output_activation: null
    dropout_prob: 0.0 
    layer_norm: false
  init_std: 1.0
  min_std: 0.00001
  max_std: 10.0
  std_type: 'heteroscedastic'
  learn_logvar_bounds: false
  use_tanh: true

critic:
  mlp_params:
    hidden_layers: [256, 256] 
    activation: torch.nn.ReLU 
    output_activation: null
    dropout_prob: 0.0 
    layer_norm: false

world_model:
  mlp_params:
    hidden_layers: [256, 256] 
    activation: torch.nn.ReLU 
    output_activation: null
    dropout_prob: 0.0 
    layer_norm: false
  init_std: 1.0
  min_std: 0.00001
  max_std: 10.0
  std_type: 'heteroscedastic'
  learn_logvar_bounds: false
  learn_reward: true
  learn_termination: false



agent:
  name: 'MPQ'
  num_train_steps: 1000
  num_steps_per_env: 8
  num_update_steps: 8
  discount: 0.99
  train_batch_size: 256
  num_action_samples: 32
  log_freq: 50
  checkpoint_freq: 500
  eval_freq: 100
  num_eval_episodes: 10
  fixed_alpha: 0.2
  init_log_alpha: 0.0
  automatic_entropy_tuning: true
  policy_optimizer:
    __target__: torch.optim.Adam
    lr: 0.0003
  critic_optimizer:
    __target__: torch.optim.Adam
    lr: 0.0003
  world_model_optimizer:
    __target__: torch.optim.Adam
    lr: 0.0003
  alpha_optimizer:
    __target__: torch.optim.Adam
    lr: 0.0003
  polyak_tau: 0.005
  target_update_interval: 1
  model_based_critic_updates: true
  rollout_horizon: 10
  num_rollouts: 5

eval:
  checkpoint: './tmp_results/${task_name}/${train.agent.name}/models/agent_checkpoint_0.pt'
  num_eval_episodes: 10
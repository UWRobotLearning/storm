
dataset_path: 'policy_eval/data/mpc_buffer_0.pt'
data_folder: 'policy_eval/data/mpc'

policy:
  mlp_params:
    hidden_layers: [256, 256] 
    activation: torch.nn.ReLU 
    output_activation: null
    dropout_prob: 0.0 
    layer_norm: false
  init_std: 1.0
  min_std: 0.00001
  max_std: 10.0
  std_type: 'heteroscedastic'
  learn_logvar_bounds: false
  use_tanh: false

qf:
  mlp_params:
    hidden_layers: [256, 256] 
    activation: torch.nn.ReLU 
    output_activation: null
    dropout_prob: 0.0 
    layer_norm: false
  ensemble_size: 2
  prediction_size: 2
  aggregation: 'max'

vf:
  mlp_params:
    hidden_layers: [256, 256] 
    activation: torch.nn.ReLU 
    output_activation: null #torch.nn.Tanh
    dropout_prob: 0.01
    layer_norm: true
  ensemble_size: 100
  aggregation: 'None'
  prediction_temp: 30.0 
  prior_factor: 0.0
  w_init: 0.0 #3e-3

agent:
  name: 'BP'
  max_buffer_size: 1e6
  num_train_epochs: 350
  train_batch_size: 256 #256
  terminal_batch_size: 100 #64 #TODO: increase this
  train_val_split_ratio: 0.95
  policy_optimizer:
    __target__: torch.optim.Adam
    lr: 3e-4
  qf_optimizer:
    __target__: torch.optim.Adam
    lr: 3e-4
  vf_optimizer:
    __target__: torch.optim.Adam
    lr: 3e-4
  polyak_tau: 5e-3
  log_freq: 100
  checkpoint_freq: 1
  eval_freq: 20000
  eval_first_iter: false
  validation_freq: 20
  num_eval_episodes: 3
  policy_use_tanh: ${train.policy.use_tanh}
  num_warmstart_steps: 200000
  num_action_samples: 1
  fixed_alpha: 0.2
  relabel_data: true
  load_pretrained_policy: false
  discount: 0.0 #0.99
  beta: 3.0
  expectile_tau: 0.7
  clip_values: false
  normalize_rewards: false
  normalize_observations: true 
  normalize_returns: true
  randomize_ensemble_targets: true
  use_target_networks: false
  lambd: 0.0
